# ---------- src/Makevars.win (CPU-only, force-include version header) ----------
$(info >>> Using Makevars.win)
$(info >>> CC=$(CC))
$(info >>> CXX=$(CXX))

# Force-include ggml-version.h so ggml.c always sees GGML_VERSION / GGML_COMMIT
PKG_CPPFLAGS = -I. -I./ggml-cpu -include ggml-version.h \
  -DGGML_USE_CPU -DGGML_CPU_GENERIC -DGGML_USE_K_QUANTS

PKG_CFLAGS   = -O3 -DNDEBUG -Wall -Wextra -Wno-unused-function
PKG_CXXFLAGS = -O3 -DNDEBUG -std=gnu++17 -Wall -Wextra -Wno-unused-function
PKG_LIBS     = -lm

# ---------------- sources we actually need (no ggml-opt.cpp) ----------------
CORE_C = \
  ggml.c ggml-alloc.c ggml-quants.c quants.c ggml-cpu.c

CORE_CPP = \
  ggml-backend.cpp ggml-threading.cpp \
  gguf.cpp unicode.cpp unicode-data.cpp vec.cpp \
  binary-ops.cpp unary-ops.cpp ops.cpp traits.cpp hbm.cpp repack.cpp \
  llama.cpp llama-arch.cpp llama-batch.cpp llama-chat.cpp llama-context.cpp llama-cparams.cpp \
  llama-grammar.cpp llama-graph.cpp llama-hparams.cpp llama-impl.cpp llama-io.cpp \
  llama-kv-cache.cpp llama-kv-cache-iswa.cpp llama-memory.cpp \
  llama-memory-hybrid.cpp llama-memory-recurrent.cpp \
  llama-mmap.cpp llama-model.cpp llama-model-loader.cpp llama-model-saver.cpp \
  llama-quant.cpp llama-sampling.cpp llama-vocab.cpp llama-adapter.cpp

# your package-facing files
PKG_CPP = interface.cpp cpu_shim.cpp

LIBOBJS = $(CORE_C:.c=.o) $(CORE_CPP:.cpp=.o)
OBJECTS = $(PKG_CPP:.cpp=.o)
LIBNAME = libllama.a

$(info >>> LIBOBJS=$(LIBOBJS))
$(info >>> OBJECTS=$(OBJECTS))
$(info >>> SHLIB=$(SHLIB))

# ---------------- rules (use ALL_* so PKG_* flags propagate) ----------------
%.o: %.c
	$(CC)  $(ALL_CFLAGS)   $(ALL_CPPFLAGS) -c $< -o $@

%.o: %.cpp
	$(CXX) $(ALL_CXXFLAGS) $(ALL_CPPFLAGS) -c $< -o $@

$(LIBNAME): $(LIBOBJS)
	$(AR) rcs $@ $(LIBOBJS)

all: $(SHLIB)

$(SHLIB): $(LIBNAME) $(OBJECTS)
	$(SHLIB_LINK) -o $@ $(OBJECTS) $(LIBNAME) $(PKG_LIBS)

clean:
	-del *.o *.a *.dll 2>nul || rm -f *.o *.a *.dll
