# ---------- src/Makevars.win (CPU-only; ggml-backend.cpp) ----------
.DEFAULT_GOAL := all
.PHONY: all
all: $(SHLIB)

$(info >>> Using Makevars.win)
$(info >>> CC=$(CC))
$(info >>> CXX=$(CXX))

PKG_CPPFLAGS = -I. -I./ggml-cpu -include ggml-version.h \
  -DGGML_USE_CPU -DGGML_CPU_GENERIC -DGGML_USE_K_QUANTS
PKG_CFLAGS   = -O3 -DNDEBUG -Wall -Wextra -Wno-unused-function
PKG_CXXFLAGS = -O3 -DNDEBUG -std=gnu++17 -Wall -Wextra -Wno-unused-function
PKG_LIBS     = $(LIBNAME) -lm

# ---- Core sources ----
CORE_C = \
  ggml.c ggml-alloc.c ggml-quants.c quants.c

CORE_CPP = \
  ggml-cpu.cpp ggml-backend.cpp ggml-threading.cpp ggml-opt.cpp \
  gguf.cpp unicode.cpp unicode-data.cpp vec.cpp \
  binary-ops.cpp unary-ops.cpp ops.cpp traits.cpp hbm.cpp repack.cpp \
  llama.cpp llama-arch.cpp llama-batch.cpp llama-chat.cpp llama-context.cpp llama-cparams.cpp \
  llama-grammar.cpp llama-graph.cpp llama-hparams.cpp llama-impl.cpp llama-io.cpp \
  llama-kv-cache.cpp llama-kv-cache-iswa.cpp llama-memory.cpp \
  llama-memory-hybrid.cpp llama-memory-recurrent.cpp \
  llama-mmap.cpp llama-model.cpp llama-model-loader.cpp llama-model-saver.cpp \
  llama-quant.cpp llama-sampling.cpp llama-vocab.cpp llama-adapter.cpp

OBJECTS = interface.o cpu_shim.o
LIBOBJS = $(CORE_C:.c=.o) $(CORE_CPP:.cpp=.o)
LIBNAME = libllama.a

$(info >>> LIBOBJS=$(LIBOBJS))
$(info >>> OBJECTS=$(OBJECTS))
$(info >>> SHLIB=$(SHLIB))

%.o: %.c
	$(CC)  $(ALL_CFLAGS)   $(ALL_CPPFLAGS) -c $< -o $@

%.o: %.cpp
	$(CXX) $(ALL_CXXFLAGS) $(ALL_CPPFLAGS) -c $< -o $@

$(LIBNAME): $(LIBOBJS)
	$(AR) rcs $@ $(LIBOBJS)

$(SHLIB): $(LIBNAME)

clean:
	-del *.o *.a *.dll 2>nul || rm -f *.o *.a *.dll
