j <- resp |> resp_body_json()
str(j)
request(EMBED_ENDPOINT) |>
req_body_json(list(model = "embeddinggemma", input = "hello world")) |>
req_perform()
resp <- request(EMBED_ENDPOINT) |>
req_body_json(list(model = "embeddinggemma", input = "hello world")) |>
req_perform()
j <- resp |> resp_body_json()
if (!is.null(j$embedding) && length(j$embedding) > 0) {
message("Got embedding: length = ", length(j$embedding))
} else {
stop("No embedding returned. Model may not support embeddings in this Ollama setup.")
}
EMBED_MODEL <- "qwen3-embedding"
# =========================================================
# 3) Embeddings (no translation) + choose k (Gap) + kmeans
#    Also compute Elbow & Silhouette for reference
# =========================================================
emb_inputs <- texts
emb_inputs
emb_matrix <- ollama_embed(EMBED_MODEL, emb_inputs)
emb_matrix
EMBED_MODEL
emb_inputs
ollama_embed
EMBED_MODEL <- "qwen3-embedding"   # change here if your local model name differs
ollama_embed <- function(model, inputs) {
# inputs: character vector
# returns a numeric matrix (rows = inputs)
stopifnot(is.character(inputs))
# Ollama embeddings endpoint expects a single string or an array depending on model/server version.
# We'll call it once per input to be robust & stream progress.
pb <- progress_bar$new(
format = "  Embedding [:bar] :current/:total (:percent) eta: :eta",
total = length(inputs), clear = FALSE, width = 60
)
embs <- vector("list", length(inputs))
for (i in seq_along(inputs)) {
body <- list(model = model, input = inputs[i])
resp <- request(EMBED_ENDPOINT) |>
req_body_json(body) |>
req_perform()
j <- resp |> resp_body_json()
# some servers return list(embedding = [...]); others 'embeddings' per batch
vec <- j$embedding %||% j$embeddings[[1]]
embs[[i]] <- unlist(vec, use.names = FALSE)
pb$tick()
}
# bind to matrix
maxlen <- max(lengths(embs))
# pad if needed (shouldn't happen, but safe)
embs <- lapply(embs, function(v) { length(v) <- maxlen; v[is.na(v)] <- 0; v })
do.call(rbind, embs)
}
emb_matrix <- ollama_embed(EMBED_MODEL, emb_inputs)
model=EMBED_MODEL
inputs=emb_inputs
model
model='embeddinggemma:latest'
i
body <- list(model = model, input = inputs[i])
resp <- request(EMBED_ENDPOINT) |>
req_body_json(body) |>
req_perform()
resp
j <- resp |> resp_body_json()
j
resp
str(resp)
model
EMBED_MODEL <- "embeddinggemma:latest"
# =========================================================
# 3) Embeddings (no translation) + choose k (Gap) + kmeans
#    Also compute Elbow & Silhouette for reference
# =========================================================
emb_inputs <- texts
emb_matrix <- ollama_embed(EMBED_MODEL, emb_inputs)
emb_matrix
# inputs: character vector
# returns a numeric matrix (rows = inputs)
stopifnot(is.character(inputs))
# Ollama embeddings endpoint expects a single string or an array depending on model/server version.
# We'll call it once per input to be robust & stream progress.
pb <- progress_bar$new(
format = "  Embedding [:bar] :current/:total (:percent) eta: :eta",
total = length(inputs), clear = FALSE, width = 60
)
embs <- vector("list", length(inputs))
EMBED_ENDPOINT
req_body_json(body)
request(EMBED_ENDPOINT) |>
req_body_json(body) |>
req_perform()
resp <- request(EMBED_ENDPOINT) |>
req_body_json(body) |>
req_perform()
resp
resp |> resp_body_json()
EMBED_ENDPOINT
body
resp <- request(EMBED_ENDPOINT) |>
req_body_json(body) |>
req_perform()
resp |> resp_body_json()
body <- list(model = model, prompt = inputs[i])
resp <- request(EMBED_ENDPOINT) |>
req_body_json(body) |>
req_perform()
resp |> resp_body_json()
j <- resp |> resp_body_json()
# some servers return list(embedding = [...]); others 'embeddings' per batch
vec <- j$embedding %||% j$embeddings[[1]]
embs[[i]] <- unlist(vec, use.names = FALSE)
embs[[i]]
ollama_embed <- function(model, inputs) {
# inputs: character vector
# returns a numeric matrix (rows = inputs)
stopifnot(is.character(inputs))
# Ollama embeddings endpoint expects a single string or an array depending on model/server version.
# We'll call it once per input to be robust & stream progress.
pb <- progress_bar$new(
format = "  Embedding [:bar] :current/:total (:percent) eta: :eta",
total = length(inputs), clear = FALSE, width = 60
)
embs <- vector("list", length(inputs))
for (i in seq_along(inputs)) {
body <- list(model = model, prompt = inputs[i])
resp <- request(EMBED_ENDPOINT) |>
req_body_json(body) |>
req_perform()
j <- resp |> resp_body_json()
# some servers return list(embedding = [...]); others 'embeddings' per batch
vec <- j$embedding %||% j$embeddings[[1]]
embs[[i]] <- unlist(vec, use.names = FALSE)
pb$tick()
}
# bind to matrix
maxlen <- max(lengths(embs))
# pad if needed (shouldn't happen, but safe)
embs <- lapply(embs, function(v) { length(v) <- maxlen; v[is.na(v)] <- 0; v })
do.call(rbind, embs)
}
emb_matrix <- ollama_embed(EMBED_MODEL, emb_inputs)
# --- Elbow method (k = 1..Kmax) ---
Kmax <- min(12, max(3, floor(sqrt(nrow(emb_matrix)))))  # a reasonable cap
wcss <- numeric(Kmax)
set.seed(SEED)
for (k in 1:Kmax) {
km <- kmeans(emb_matrix, centers = k, nstart = 5, iter.max = 50)
wcss[k] <- km$tot.withinss
}
# compute "elbow" by max perpendicular distance to line from (1, wcss1) to (Kmax, wcssK)
line_dist <- function(k_vals, y_vals) {
x1 <- k_vals[1]; y1 <- y_vals[1]
x2 <- k_vals[length(k_vals)]; y2 <- y_vals[length(y_vals)]
denom <- sqrt((x2 - x1)^2 + (y2 - y1)^2)
sapply(seq_along(k_vals), function(i) {
xi <- k_vals[i]; yi <- y_vals[i]
abs((y2 - y1)*xi - (x2 - x1)*yi + x2*y1 - y2*x1) / denom
})
}
elbow_k <- which.max(line_dist(1:Kmax, wcss))
# --- Silhouette (k >= 2) ---
sil_avgs <- rep(NA_real_, Kmax)
d <- dist(emb_matrix)
for (k in 2:Kmax) {
km <- kmeans(emb_matrix, centers = k, nstart = 5, iter.max = 50)
sil <- silhouette(km$cluster, d)
sil_avgs[k] <- mean(sil[, "sil_width"])
}
silhouette_k <- which.max(sil_avgs)
# --- Gap statistic (Tibshirani) ---
set.seed(SEED)
gap <- clusGap(emb_matrix, FUN = function(x, k) kmeans(x, centers = k, nstart = 5, iter.max = 50),
K.max = Kmax, B = 20)  # increase B for more stability if desired
gap_df <- as_tibble(gap$Tab)
gap_k <- which.max(gap_df$gap)
cat("\n--- Cluster diagnostics ---\n")
cat(sprintf("Elbow k (max distance): %d\n", elbow_k))
cat(sprintf("Silhouette k (max avg): %d\n", silhouette_k))
cat(sprintf("Gap statistic k (argmax gap): %d\n", gap_k))
# Use Gap statistic choice for final k
final_k <- gap_k
set.seed(SEED)
km_final <- kmeans(emb_matrix, centers = final_k, nstart = 25, iter.max = 100)
cluster_labels <- km_final$cluster
# One-hot encode clusters
cluster_cols <- paste0("KMEANS_C", sprintf("%02d", 1:final_k))
clu_mat <- matrix(0L, nrow = length(cluster_labels), ncol = final_k)
for (i in seq_along(cluster_labels)) {
clu_mat[i, cluster_labels[i]] <- 1L
}
colnames(clu_mat) <- cluster_cols
clu_df <- as_tibble(clu_mat)
# =========================================================
# 4) Final dataset
# =========================================================
final_df <- df |>
select(UniqueID, ACAN_ELECTION_FREENEXT_COAL) |>
bind_cols(cls_df) |>
bind_cols(clu_df)
final_df
names(final_df)
# Print a small preview & save
print(head(final_df, 5))
readr::write_csv(final_df, "tides_wave6_llm_and_clusters.csv")
cat("\nSaved: tides_wave6_llm_and_clusters.csv\n")
library(torch)
library(fastrtext)
library(torch)
library(sentencepiece)
library(torch)
# Load TorchScript model
model <- jit_load("/Users/justinsavoie/Downloads/distiluse_embed.pt")
library(torch)
library(sentencepiece)
model <- jit_load("distiluse_embed.pt")
model <- jit_load("/Users/justinsavoie/Downloads/distiluse_embed.pt")
model <- jit_load("/Users/justinsavoie/Downloads/xlmr_embed.pt")
sp <- sentencepiece_load_model("~/Downloads/sentencepiece.bpe.model")
sp <- sentencepiece_load_model("~/Downloads/sentencepiece.bpe.model")
sp <- sentencepiece_load_model("/Users/justinsavoie/Downloads/sentencepiece.bpe.model")
library(torch)
library(sentencepiece)
model <- jit_load("/Users/justinsavoie/Downloads/xlmr_embed.pt")
sp <- sentencepiece_load_model("/Users/justinsavoie/Downloads/sentencepiece.bpe.model")
texts <- c(
"We must invest more in healthcare and education.",
"Cut taxes for small businesses now."
)
# Tokenize to tensors
tokenize_to_tensors <- function(texts, max_len = 64L, pad_id = 1L) {
enc <- sentencepiece_encode(sp, texts, out_type = "ids")
padded <- lapply(enc, function(ids) {
ids <- as.integer(ids)
if (length(ids) > max_len) ids <- ids[1:max_len]
c(ids, rep(pad_id, max_len - length(ids)))
})
attn <- lapply(padded, function(ids) as.integer(ids != pad_id))
input_ids <- do.call(rbind, padded)
attention <- do.call(rbind, attn)
list(
input_ids = torch_tensor(input_ids, dtype = torch_long()),
attention = torch_tensor(attention, dtype = torch_long())
)
}
tok <- tokenize_to_tensors(texts)
tok <- tokenize_to_tensors(texts)
# Tokenize to tensors
tokenize_to_tensors <- function(texts, max_len = 64L, pad_id = 1L) {
# Encode to integer IDs
enc <- sentencepiece_encode(sp, texts)  # returns list of integer vectors
# Pad / truncate
padded <- lapply(enc, function(ids) {
ids <- as.integer(ids)
if (length(ids) > max_len) ids <- ids[1:max_len]
c(ids, rep(pad_id, max_len - length(ids)))
})
# Attention mask: 1 for non-pad, 0 for pad
attn <- lapply(padded, function(ids) as.integer(ids != pad_id))
input_ids <- do.call(rbind, padded)
attention <- do.call(rbind, attn)
list(
input_ids = torch_tensor(input_ids, dtype = torch_long()),
attention = torch_tensor(attention, dtype = torch_long())
)
}
texts <- c(
"We must invest more in healthcare and education.",
"Cut taxes for small businesses now."
)
tok <- tokenize_to_tensors(texts)
with_no_grad({
emb <- model(tok$input_ids, tok$attention)
})
emb$shape   # should give [2, 768]
tok <- tokenize_to_tensors(texts)
tok <- tokenize_to_tensors(texts)
with_no_grad({
emb <- model(tok$input_ids, tok$attention)
})
# Tokenize to tensors
tokenize_to_tensors <- function(texts, max_len = 64L, pad_id = 1L) {
# Encode to integer IDs
enc <- sentencepiece_encode(sp, texts, type = "id")  # returns list of integer vectors
# Pad / truncate
padded <- lapply(enc, function(ids) {
ids <- as.integer(ids)
if (length(ids) > max_len) ids <- ids[1:max_len]
c(ids, rep(pad_id, max_len - length(ids)))
})
# Attention mask: 1 for non-pad, 0 for pad
attn <- lapply(padded, function(ids) as.integer(ids != pad_id))
input_ids <- do.call(rbind, padded)
attention <- do.call(rbind, attn)
list(
input_ids = torch_tensor(input_ids, dtype = torch_long()),
attention = torch_tensor(attention, dtype = torch_long())
)
}
texts <- c(
"We must invest more in healthcare and education.",
"Cut taxes for small businesses now."
)
tok <- tokenize_to_tensors(texts)
with_no_grad({
emb <- model(tok$input_ids, tok$attention)
})
emb$shape   # should give [2, 768]
emb
library(torch)
library(sentencepiece)
model <- jit_load("/Users/justinsavoie/Downloads/xlmr_embed.pt")
sp <- sentencepiece_load_model("/Users/justinsavoie/Downloads/sentencepiece.bpe.model")
# Tokenize to tensors
tokenize_to_tensors <- function(texts, max_len = 64L, pad_id = 1L) {
# Encode to integer IDs
enc <- sentencepiece_encode(sp, texts, type = "id")  # returns list of integer vectors
# Pad / truncate
padded <- lapply(enc, function(ids) {
ids <- as.integer(ids)
if (length(ids) > max_len) ids <- ids[1:max_len]
c(ids, rep(pad_id, max_len - length(ids)))
})
# Attention mask: 1 for non-pad, 0 for pad
attn <- lapply(padded, function(ids) as.integer(ids != pad_id))
input_ids <- do.call(rbind, padded)
attention <- do.call(rbind, attn)
list(
input_ids = torch_tensor(input_ids, dtype = torch_long()),
attention = torch_tensor(attention, dtype = torch_long())
)
}
texts <- c(
"We must invest more in healthcare and education.",
"Cut taxes for small businesses now."
)
tok <- tokenize_to_tensors(texts)
with_no_grad({
emb <- model(tok$input_ids, tok$attention)
})
emb
texts <- c(
"This is a text"
)
tok <- tokenize_to_tensors(texts)
with_no_grad({
emb <- model(tok$input_ids, tok$attention)
})
emb
c(emb)
emb[[1]]
emb$abs
str(emb)
emb[1,]
c(emb[1,])
str(emb[1,])
emb[1,]
as.numeric(emb[1,])
as.numeric(emb[1, 1:10])
texts <- c(
"this is a TEST"
)
tok <- tokenize_to_tensors(texts)
with_no_grad({
emb <- model(tok$input_ids, tok$attention)
})
as.numeric(emb[1,])
as.numeric(emb[1, 1:10])
texts <- c(
"this is a TEST"
)
tok <- tokenize_to_tensors(texts)
with_no_grad({
emb <- model(tok$input_ids, tok$attention)
})
as.numeric(emb[1, 1:10])
sentencepiece_encode(sp, "this is a TEST", type="id")
tokenize_to_tensors <- function(texts, max_len = 64L, pad_id = 1L) {
enc <- sentencepiece_encode(sp, texts, type = "id")
padded <- lapply(enc, function(ids) {
# shift +1 like Hugging Face
ids <- as.integer(ids) + 1L
# add [CLS]=0 at start, [SEP]=2 at end
ids <- c(0L, ids, 2L)
if (length(ids) > max_len) ids <- ids[1:max_len]
c(ids, rep(pad_id, max_len - length(ids)))
})
attn <- lapply(padded, function(ids) as.integer(ids != pad_id))
list(
input_ids = torch_tensor(do.call(rbind, padded), dtype = torch_long()),
attention = torch_tensor(do.call(rbind, attn), dtype = torch_long())
)
}
texts <- c(
"this is a TEST"
)
tok <- tokenize_to_tensors(texts)
with_no_grad({
emb <- model(tok$input_ids, tok$attention)
})
as.numeric(emb[1, 1:10])
library(httr)
library(jsonlite)
res <- POST(
"http://localhost:11434/api/generate",
body = toJSON(list(
model = "llama3",
prompt = "Summarize R's philosophy in one sentence.",
temperature = 0,
stream = FALSE
), auto_unbox = TRUE),
encode = "raw",
content_type_json()
)
res <- POST(
"http://localhost:11434/api/generate",
body = toJSON(list(
model = "gemma3n:latest",
prompt = "Summarize R's philosophy in one sentence.",
temperature = 0,
stream = FALSE
), auto_unbox = TRUE),
encode = "raw",
content_type_json()
)
cat(fromJSON(content(res, "text"))$response)
content(res, "text"))$response
content(res, "text")
fromJSON(content(res, "text"))
fromJSON(content(res, "text"))$response
cat(fromJSON(content(res, "text"))$response)
res <- POST(
"http://localhost:11434/api/generate",
body = toJSON(list(
model = "gemma3n:latest",
prompt = "Summarize Rorty's philosophy in a short paragraph.",
temperature = 0,
stream = FALSE
), auto_unbox = TRUE),
encode = "raw",
content_type_json()
)
cat(fromJSON(content(res, "text"))$response)
llamar::llama_build_test()
llamar::llama_generate_greedy("/Users/justinsavoie/Documents/personal_repos/ai_creative/llm-playground/mistral-7b-instruct-q4_k_m.gguf", "Hello, my name
is", 16L, n_ctx = 256L)
llamar::llama_generate_greedy("/Users/justinsavoie/Documents/personal_repos/ai_creative/llm-playground/mistral-7b-instruct-q4_k_m.gguf", "Hello, my name
is", 16L)
225L
llamar::llama_generate_greedy("/Users/justinsavoie/Documents/personal_repos/ai_creative/llm-playground/mistral-7b-instruct-q4_k_m.gguf",
"Summarize Rorty's philosophy in a short paragraph.", 225L)
llamar::llama_generate_greedy("/Users/justinsavoie/Documents/personal_repos/ai_creative/llm-playground/mistral-7b-instruct-q4_k_m.gguf",
"Summarize Rorty's philosophy in one sentence.", 225L)
?llamar::llama_generate_greedy
llamar::llama_generate_greedy
library(llamar)
library(llamar)
library(llamar)
llamar::llama_build_test()  # expect “Success!”
Sys.setenv(LLAMAR_USE_MMAP = "0")  # keep this off for macOS stability
model <- path.expand("~/Documents/personal_repos/ai_creative/llm-playground/mistral-7b-instruct-q4_k_m.gguf")
txt <- llamar::llama_generate_greedy(model, "Hello", 32L, 128L)
txt
txt <- llamar::llama_generate_greedy(model, "Summarise Richard Rorty's philosophy.", 32L, 128L)
txt
txt <- llamar::llama_generate_greedy(model, "Summarise Richard Rorty's philosophy.", 32L, 1000)
txt
txt <- llamar::llama_generate_greedy(model, "Summarise Richard Rorty's philosophy.", 1000, 1000)
txt
cat(txt)
txt <- llamar::llama_generate_greedy(model, "Summarise Richard Rorty's philosophy.", 300, 1000)
cat(txt)
remove.packages("llamar")
library(llamar)
remove.packages("llamar")
library(llamar)
remove.packages("llamar")
library(llamarddffcx)
library(llamar)
remove.packages("llamar")
library(llamar)
llamar::llama_build_test()  # expect “Success!”
Sys.setenv(LLAMAR_USE_MMAP = "0")  # keep this off for macOS stability
prompt
Sys.setenv(LLAMAR_USE_MMAP = "0")
model <- path.expand("~/Downloads/mistral-7b-instruct-v0.3-q4_k_m.gguf")
prompt <- readChar("~/Desktop/t.txt", file.info("~/Desktop/t.txt")$size)
prompt
library(tidyverse)
library(tidyverse)
str_sub(prompt,1,100)
str_sub(prompt,1,100)
str_sub(prompt,1,300)
txt <- llamar::llama_generate_greedy(model, str_sub(prompt,1,300), 500, 1000)
cat(txt)
str_sub(prompt,1,300)
str_sub(prompt,1,1000)
str_sub(prompt,1,2000)
llamar::llama_generate_greedy
str_sub(prompt,1,2000)
txt <- llamar::llama_generate_greedy(model, str_sub(prompt,1,2000), 500, 1000)
cat(txt)
str_sub(prompt,1,2000)
getwd()
setwd("~/Documents/personal_repos/ai_creative/llm-playground/llamaproject/llamar/")
list.files("src")
list.files("src/ggml-cpu/")
remove.packages("llamar")
remove.packages("llamar")
remove.packages("llamar")
